{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def func(hw,Nmax):\n",
    "    c0 = 10.0\n",
    "    return Nmax**2\n",
    "    #return hw+Nmax*2.0\n",
    "    #return (1-np.exp(-hw*Nmax*10))*c0\n",
    "\n",
    "# Generate all of the data:\n",
    "hw = np.arange(5,50,1)\n",
    "Nmax = np.arange(10,50,2)\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "# Generate all of the data\n",
    "for hwi in hw:\n",
    "    for Nmaxi in Nmax:\n",
    "        xi=[hwi*1.0,Nmaxi*1.0]\n",
    "        yi=func(hwi,Nmaxi)\n",
    "        X.append(xi)\n",
    "        Y.append(yi)\n",
    "\n",
    "X=np.asarray(X)\n",
    "Y=np.asarray(Y)\n",
    "\n",
    "# Train/Test Split\n",
    "X_train,X_test,Y_train,Y_test =train_test_split(X,Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900, 2)\n",
      "(180, 2)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 100)               300       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 10,501\n",
      "Trainable params: 10,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Now we create the model\n",
    "model = Sequential()\n",
    "model.add(Dense(100,input_dim=2,kernel_initializer='normal',activation='relu'))\n",
    "model.add(Dense(100,input_dim=2,kernel_initializer='normal',activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='normal'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 576 samples, validate on 144 samples\n",
      "Epoch 1/200\n",
      "576/576 [==============================] - 0s 303us/step - loss: 7.5583 - val_loss: 6.2414\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 6.24139, saving model to best.model.hdf5\n",
      "Epoch 2/200\n",
      "576/576 [==============================] - 0s 178us/step - loss: 5.2282 - val_loss: 6.1681\n",
      "\n",
      "Epoch 00002: val_loss improved from 6.24139 to 6.16813, saving model to best.model.hdf5\n",
      "Epoch 3/200\n",
      "576/576 [==============================] - 0s 178us/step - loss: 5.4673 - val_loss: 8.6726\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 6.16813\n",
      "Epoch 4/200\n",
      "576/576 [==============================] - 0s 177us/step - loss: 6.0045 - val_loss: 6.1642\n",
      "\n",
      "Epoch 00004: val_loss improved from 6.16813 to 6.16417, saving model to best.model.hdf5\n",
      "Epoch 5/200\n",
      "576/576 [==============================] - 0s 160us/step - loss: 8.9483 - val_loss: 16.0762\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 6.16417\n",
      "Epoch 6/200\n",
      "576/576 [==============================] - 0s 178us/step - loss: 17.1912 - val_loss: 22.8436\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 6.16417\n",
      "Epoch 7/200\n",
      "576/576 [==============================] - 0s 145us/step - loss: 13.7743 - val_loss: 5.9910\n",
      "\n",
      "Epoch 00007: val_loss improved from 6.16417 to 5.99099, saving model to best.model.hdf5\n",
      "Epoch 8/200\n",
      "576/576 [==============================] - 0s 180us/step - loss: 8.1248 - val_loss: 7.5163\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 5.99099\n",
      "Epoch 9/200\n",
      "576/576 [==============================] - 0s 194us/step - loss: 5.7838 - val_loss: 8.2252\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 5.99099\n",
      "Epoch 10/200\n",
      "576/576 [==============================] - 0s 158us/step - loss: 5.2140 - val_loss: 7.3488\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 5.99099\n",
      "Epoch 11/200\n",
      "576/576 [==============================] - 0s 207us/step - loss: 5.4426 - val_loss: 7.2051\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 5.99099\n",
      "Epoch 12/200\n",
      "576/576 [==============================] - 0s 161us/step - loss: 4.7646 - val_loss: 6.1576\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 5.99099\n",
      "Epoch 13/200\n",
      "576/576 [==============================] - 0s 164us/step - loss: 4.1922 - val_loss: 6.3696\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 5.99099\n",
      "Epoch 14/200\n",
      "576/576 [==============================] - 0s 181us/step - loss: 4.6410 - val_loss: 5.7917\n",
      "\n",
      "Epoch 00014: val_loss improved from 5.99099 to 5.79174, saving model to best.model.hdf5\n",
      "Epoch 15/200\n",
      "576/576 [==============================] - 0s 154us/step - loss: 5.4342 - val_loss: 7.8451\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 5.79174\n",
      "Epoch 16/200\n",
      "576/576 [==============================] - 0s 201us/step - loss: 5.8924 - val_loss: 6.7935\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 5.79174\n",
      "Epoch 17/200\n",
      "576/576 [==============================] - 0s 200us/step - loss: 5.1555 - val_loss: 6.3380\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 5.79174\n",
      "Epoch 18/200\n",
      "576/576 [==============================] - 0s 213us/step - loss: 5.2918 - val_loss: 7.7952\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 5.79174\n",
      "Epoch 19/200\n",
      "576/576 [==============================] - 0s 190us/step - loss: 4.6602 - val_loss: 5.9979\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 5.79174\n",
      "Epoch 20/200\n",
      "576/576 [==============================] - 0s 180us/step - loss: 5.7827 - val_loss: 5.3603\n",
      "\n",
      "Epoch 00020: val_loss improved from 5.79174 to 5.36034, saving model to best.model.hdf5\n",
      "Epoch 21/200\n",
      "576/576 [==============================] - 0s 182us/step - loss: 5.5264 - val_loss: 5.6381\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 5.36034\n",
      "Epoch 22/200\n",
      "576/576 [==============================] - 0s 181us/step - loss: 6.0114 - val_loss: 7.1951\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 5.36034\n",
      "Epoch 23/200\n",
      "576/576 [==============================] - 0s 190us/step - loss: 5.8698 - val_loss: 6.3931\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 5.36034\n",
      "Epoch 24/200\n",
      "576/576 [==============================] - 0s 187us/step - loss: 6.2644 - val_loss: 5.8077\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 5.36034\n",
      "Epoch 25/200\n",
      "576/576 [==============================] - 0s 214us/step - loss: 7.2662 - val_loss: 7.4311\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 5.36034\n",
      "Epoch 26/200\n",
      "576/576 [==============================] - 0s 167us/step - loss: 6.8198 - val_loss: 6.2343\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 5.36034\n",
      "Epoch 27/200\n",
      "576/576 [==============================] - 0s 155us/step - loss: 4.8405 - val_loss: 5.1474\n",
      "\n",
      "Epoch 00027: val_loss improved from 5.36034 to 5.14742, saving model to best.model.hdf5\n",
      "Epoch 28/200\n",
      "576/576 [==============================] - 0s 173us/step - loss: 3.7147 - val_loss: 5.8386\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 5.14742\n",
      "Epoch 29/200\n",
      "576/576 [==============================] - 0s 195us/step - loss: 5.1242 - val_loss: 7.2778\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 5.14742\n",
      "Epoch 30/200\n",
      "576/576 [==============================] - 0s 231us/step - loss: 4.7695 - val_loss: 5.4962\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 5.14742\n",
      "Epoch 31/200\n",
      "576/576 [==============================] - 0s 231us/step - loss: 6.4550 - val_loss: 13.3463\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 5.14742\n",
      "Epoch 32/200\n",
      "576/576 [==============================] - 0s 190us/step - loss: 12.8156 - val_loss: 22.4898\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 5.14742\n",
      "Epoch 33/200\n",
      "576/576 [==============================] - 0s 178us/step - loss: 7.8417 - val_loss: 7.1955\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 5.14742\n",
      "Epoch 34/200\n",
      "576/576 [==============================] - 0s 193us/step - loss: 5.9054 - val_loss: 5.1913\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 5.14742\n",
      "Epoch 35/200\n",
      "576/576 [==============================] - 0s 160us/step - loss: 3.6561 - val_loss: 5.0284\n",
      "\n",
      "Epoch 00035: val_loss improved from 5.14742 to 5.02841, saving model to best.model.hdf5\n",
      "Epoch 36/200\n",
      "576/576 [==============================] - 0s 158us/step - loss: 3.7814 - val_loss: 8.9239\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 5.02841\n",
      "Epoch 37/200\n",
      "576/576 [==============================] - 0s 136us/step - loss: 5.0156 - val_loss: 4.9559\n",
      "\n",
      "Epoch 00037: val_loss improved from 5.02841 to 4.95594, saving model to best.model.hdf5\n",
      "Epoch 38/200\n",
      "576/576 [==============================] - 0s 159us/step - loss: 5.3304 - val_loss: 10.7956\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 4.95594\n",
      "Epoch 39/200\n",
      "576/576 [==============================] - 0s 154us/step - loss: 6.2105 - val_loss: 5.1315\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 4.95594\n",
      "Epoch 40/200\n",
      "576/576 [==============================] - 0s 146us/step - loss: 4.6869 - val_loss: 15.7379\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 4.95594\n",
      "Epoch 41/200\n",
      "576/576 [==============================] - 0s 138us/step - loss: 7.8589 - val_loss: 4.8117\n",
      "\n",
      "Epoch 00041: val_loss improved from 4.95594 to 4.81175, saving model to best.model.hdf5\n",
      "Epoch 42/200\n",
      "576/576 [==============================] - 0s 150us/step - loss: 6.6420 - val_loss: 11.0270\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 4.81175\n",
      "Epoch 43/200\n",
      "576/576 [==============================] - 0s 141us/step - loss: 7.6441 - val_loss: 21.4590\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 4.81175\n",
      "Epoch 44/200\n",
      "576/576 [==============================] - 0s 161us/step - loss: 26.5486 - val_loss: 14.6894\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 4.81175\n",
      "Epoch 45/200\n",
      "576/576 [==============================] - 0s 153us/step - loss: 19.1009 - val_loss: 37.2364\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 4.81175\n",
      "Epoch 46/200\n",
      "576/576 [==============================] - 0s 174us/step - loss: 59.4049 - val_loss: 21.9269\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 4.81175\n",
      "Epoch 47/200\n",
      "576/576 [==============================] - 0s 161us/step - loss: 87.3339 - val_loss: 66.2193\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 4.81175\n",
      "Epoch 48/200\n",
      "576/576 [==============================] - 0s 189us/step - loss: 41.7614 - val_loss: 16.0304\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 4.81175\n",
      "Epoch 49/200\n",
      "576/576 [==============================] - 0s 174us/step - loss: 22.5468 - val_loss: 12.0247\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 4.81175\n",
      "Epoch 50/200\n",
      "576/576 [==============================] - 0s 139us/step - loss: 11.2256 - val_loss: 12.0292\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 4.81175\n",
      "Epoch 51/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 146us/step - loss: 7.4123 - val_loss: 10.1175\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 4.81175\n",
      "Epoch 52/200\n",
      "576/576 [==============================] - 0s 148us/step - loss: 18.4197 - val_loss: 19.3306\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 4.81175\n",
      "Epoch 53/200\n",
      "576/576 [==============================] - 0s 149us/step - loss: 23.7830 - val_loss: 11.0773\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 4.81175\n",
      "Epoch 54/200\n",
      "576/576 [==============================] - 0s 155us/step - loss: 14.2111 - val_loss: 30.0116\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 4.81175\n",
      "Epoch 55/200\n",
      "576/576 [==============================] - 0s 138us/step - loss: 8.4230 - val_loss: 7.2765\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 4.81175\n",
      "Epoch 56/200\n",
      "576/576 [==============================] - 0s 146us/step - loss: 4.9349 - val_loss: 9.1414\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 4.81175\n",
      "Epoch 57/200\n",
      "576/576 [==============================] - 0s 150us/step - loss: 4.0599 - val_loss: 4.3006\n",
      "\n",
      "Epoch 00057: val_loss improved from 4.81175 to 4.30062, saving model to best.model.hdf5\n",
      "Epoch 58/200\n",
      "576/576 [==============================] - 0s 189us/step - loss: 3.3373 - val_loss: 4.4651\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 4.30062\n",
      "Epoch 59/200\n",
      "576/576 [==============================] - 0s 179us/step - loss: 3.9643 - val_loss: 4.1393\n",
      "\n",
      "Epoch 00059: val_loss improved from 4.30062 to 4.13926, saving model to best.model.hdf5\n",
      "Epoch 60/200\n",
      "576/576 [==============================] - 0s 146us/step - loss: 2.7725 - val_loss: 4.3953\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 4.13926\n",
      "Epoch 61/200\n",
      "576/576 [==============================] - 0s 151us/step - loss: 2.9596 - val_loss: 3.9785\n",
      "\n",
      "Epoch 00061: val_loss improved from 4.13926 to 3.97846, saving model to best.model.hdf5\n",
      "Epoch 62/200\n",
      "576/576 [==============================] - 0s 135us/step - loss: 2.6651 - val_loss: 5.4655\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 3.97846\n",
      "Epoch 63/200\n",
      "576/576 [==============================] - 0s 145us/step - loss: 3.0774 - val_loss: 5.2639\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 3.97846\n",
      "Epoch 64/200\n",
      "576/576 [==============================] - 0s 160us/step - loss: 2.9385 - val_loss: 6.9350\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 3.97846\n",
      "Epoch 65/200\n",
      "576/576 [==============================] - 0s 152us/step - loss: 4.6485 - val_loss: 13.8424\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 3.97846\n",
      "Epoch 66/200\n",
      "576/576 [==============================] - 0s 142us/step - loss: 68.8101 - val_loss: 11.3490\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 3.97846\n",
      "Epoch 67/200\n",
      "576/576 [==============================] - 0s 161us/step - loss: 15.0745 - val_loss: 4.9768\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 3.97846\n",
      "Epoch 68/200\n",
      "576/576 [==============================] - 0s 143us/step - loss: 7.0722 - val_loss: 12.2896\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 3.97846\n",
      "Epoch 69/200\n",
      "576/576 [==============================] - 0s 141us/step - loss: 5.4745 - val_loss: 4.7476\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 3.97846\n",
      "Epoch 70/200\n",
      "576/576 [==============================] - 0s 145us/step - loss: 3.3918 - val_loss: 4.1943\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 3.97846\n",
      "Epoch 71/200\n",
      "576/576 [==============================] - 0s 137us/step - loss: 3.3417 - val_loss: 4.8306\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 3.97846\n",
      "Epoch 72/200\n",
      "576/576 [==============================] - 0s 148us/step - loss: 3.4944 - val_loss: 4.6546\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 3.97846\n",
      "Epoch 73/200\n",
      "576/576 [==============================] - 0s 158us/step - loss: 2.6598 - val_loss: 4.1285\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 3.97846\n",
      "Epoch 74/200\n",
      "576/576 [==============================] - 0s 148us/step - loss: 2.5604 - val_loss: 4.0995\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 3.97846\n",
      "Epoch 75/200\n",
      "576/576 [==============================] - 0s 145us/step - loss: 2.5871 - val_loss: 5.4765\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 3.97846\n",
      "Epoch 76/200\n",
      "576/576 [==============================] - 0s 154us/step - loss: 2.4538 - val_loss: 3.9115\n",
      "\n",
      "Epoch 00076: val_loss improved from 3.97846 to 3.91145, saving model to best.model.hdf5\n",
      "Epoch 77/200\n",
      "576/576 [==============================] - 0s 140us/step - loss: 3.3625 - val_loss: 4.0261\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 3.91145\n",
      "Epoch 78/200\n",
      "576/576 [==============================] - 0s 158us/step - loss: 2.9118 - val_loss: 3.8375\n",
      "\n",
      "Epoch 00078: val_loss improved from 3.91145 to 3.83745, saving model to best.model.hdf5\n",
      "Epoch 79/200\n",
      "576/576 [==============================] - 0s 130us/step - loss: 2.4670 - val_loss: 4.6984\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 3.83745\n",
      "Epoch 80/200\n",
      "576/576 [==============================] - 0s 213us/step - loss: 3.5102 - val_loss: 3.7699\n",
      "\n",
      "Epoch 00080: val_loss improved from 3.83745 to 3.76992, saving model to best.model.hdf5\n",
      "Epoch 81/200\n",
      "576/576 [==============================] - 0s 142us/step - loss: 4.1553 - val_loss: 4.6050\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 3.76992\n",
      "Epoch 82/200\n",
      "576/576 [==============================] - 0s 151us/step - loss: 6.2487 - val_loss: 4.7713\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 3.76992\n",
      "Epoch 83/200\n",
      "576/576 [==============================] - 0s 143us/step - loss: 5.6924 - val_loss: 5.6810\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 3.76992\n",
      "Epoch 84/200\n",
      "576/576 [==============================] - 0s 134us/step - loss: 3.5731 - val_loss: 4.0122\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 3.76992\n",
      "Epoch 85/200\n",
      "576/576 [==============================] - 0s 158us/step - loss: 3.4204 - val_loss: 4.7137\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 3.76992\n",
      "Epoch 86/200\n",
      "576/576 [==============================] - 0s 145us/step - loss: 4.1494 - val_loss: 3.9574\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 3.76992\n",
      "Epoch 87/200\n",
      "576/576 [==============================] - 0s 159us/step - loss: 3.3736 - val_loss: 3.6090\n",
      "\n",
      "Epoch 00087: val_loss improved from 3.76992 to 3.60902, saving model to best.model.hdf5\n",
      "Epoch 88/200\n",
      "576/576 [==============================] - 0s 157us/step - loss: 2.7229 - val_loss: 3.4800\n",
      "\n",
      "Epoch 00088: val_loss improved from 3.60902 to 3.47999, saving model to best.model.hdf5\n",
      "Epoch 89/200\n",
      "576/576 [==============================] - 0s 156us/step - loss: 3.6898 - val_loss: 4.4952\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 3.47999\n",
      "Epoch 90/200\n",
      "576/576 [==============================] - 0s 198us/step - loss: 8.0314 - val_loss: 5.0401\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 3.47999\n",
      "Epoch 91/200\n",
      "576/576 [==============================] - 0s 223us/step - loss: 6.6766 - val_loss: 3.6389\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 3.47999\n",
      "Epoch 92/200\n",
      "576/576 [==============================] - 0s 189us/step - loss: 4.9266 - val_loss: 6.6774\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 3.47999\n",
      "Epoch 93/200\n",
      "576/576 [==============================] - 0s 169us/step - loss: 3.9437 - val_loss: 4.1860\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 3.47999\n",
      "Epoch 94/200\n",
      "576/576 [==============================] - 0s 181us/step - loss: 3.2325 - val_loss: 3.7402\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 3.47999\n",
      "Epoch 95/200\n",
      "576/576 [==============================] - 0s 200us/step - loss: 2.0038 - val_loss: 3.6202\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 3.47999\n",
      "Epoch 96/200\n",
      "576/576 [==============================] - 0s 203us/step - loss: 5.9602 - val_loss: 6.1289\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 3.47999\n",
      "Epoch 97/200\n",
      "576/576 [==============================] - 0s 186us/step - loss: 11.0717 - val_loss: 4.9100\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 3.47999\n",
      "Epoch 98/200\n",
      "576/576 [==============================] - 0s 201us/step - loss: 8.5833 - val_loss: 19.8446\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 3.47999\n",
      "Epoch 99/200\n",
      "576/576 [==============================] - 0s 163us/step - loss: 25.5211 - val_loss: 57.2912\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 3.47999\n",
      "Epoch 100/200\n",
      "576/576 [==============================] - 0s 219us/step - loss: 54.9280 - val_loss: 34.4685\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 3.47999\n",
      "Epoch 101/200\n",
      "576/576 [==============================] - 0s 161us/step - loss: 21.4095 - val_loss: 23.0498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00101: val_loss did not improve from 3.47999\n",
      "Epoch 102/200\n",
      "576/576 [==============================] - 0s 156us/step - loss: 18.9047 - val_loss: 10.1455\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 3.47999\n",
      "Epoch 103/200\n",
      "576/576 [==============================] - 0s 160us/step - loss: 8.9005 - val_loss: 9.0197\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 3.47999\n",
      "Epoch 104/200\n",
      "576/576 [==============================] - 0s 156us/step - loss: 4.0796 - val_loss: 5.4026\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 3.47999\n",
      "Epoch 105/200\n",
      "576/576 [==============================] - 0s 153us/step - loss: 2.7268 - val_loss: 3.8904\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 3.47999\n",
      "Epoch 106/200\n",
      "576/576 [==============================] - 0s 163us/step - loss: 2.9869 - val_loss: 3.2377\n",
      "\n",
      "Epoch 00106: val_loss improved from 3.47999 to 3.23768, saving model to best.model.hdf5\n",
      "Epoch 107/200\n",
      "576/576 [==============================] - 0s 164us/step - loss: 3.5277 - val_loss: 3.5878\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 3.23768\n",
      "Epoch 108/200\n",
      "576/576 [==============================] - 0s 164us/step - loss: 3.4087 - val_loss: 5.1883\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 3.23768\n",
      "Epoch 109/200\n",
      "576/576 [==============================] - 0s 182us/step - loss: 4.5001 - val_loss: 15.1313\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 3.23768\n",
      "Epoch 110/200\n",
      "576/576 [==============================] - 0s 171us/step - loss: 27.3667 - val_loss: 9.3049\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 3.23768\n",
      "Epoch 111/200\n",
      "576/576 [==============================] - 0s 178us/step - loss: 6.3668 - val_loss: 3.4053\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 3.23768\n",
      "Epoch 112/200\n",
      "576/576 [==============================] - 0s 159us/step - loss: 2.7365 - val_loss: 3.6318\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 3.23768\n",
      "Epoch 113/200\n",
      "576/576 [==============================] - 0s 157us/step - loss: 3.7479 - val_loss: 11.4209\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 3.23768\n",
      "Epoch 114/200\n",
      "576/576 [==============================] - 0s 171us/step - loss: 5.8263 - val_loss: 5.8759\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 3.23768\n",
      "Epoch 115/200\n",
      "576/576 [==============================] - 0s 160us/step - loss: 6.2412 - val_loss: 11.5301\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 3.23768\n",
      "Epoch 116/200\n",
      "576/576 [==============================] - 0s 171us/step - loss: 11.6673 - val_loss: 22.1023\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 3.23768\n",
      "Epoch 117/200\n",
      "576/576 [==============================] - 0s 168us/step - loss: 11.5696 - val_loss: 13.2747\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 3.23768\n",
      "Epoch 118/200\n",
      "576/576 [==============================] - 0s 160us/step - loss: 11.7545 - val_loss: 10.4982\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 3.23768\n",
      "Epoch 119/200\n",
      "576/576 [==============================] - 0s 166us/step - loss: 4.6159 - val_loss: 5.4872\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 3.23768\n",
      "Epoch 120/200\n",
      "576/576 [==============================] - 0s 157us/step - loss: 2.1800 - val_loss: 4.2953\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 3.23768\n",
      "Epoch 121/200\n",
      "576/576 [==============================] - 0s 156us/step - loss: 3.6929 - val_loss: 4.1311\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 3.23768\n",
      "Epoch 122/200\n",
      "576/576 [==============================] - 0s 157us/step - loss: 10.5062 - val_loss: 8.4151\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 3.23768\n",
      "Epoch 123/200\n",
      "576/576 [==============================] - 0s 154us/step - loss: 3.1391 - val_loss: 3.2795\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 3.23768\n",
      "Epoch 124/200\n",
      "576/576 [==============================] - 0s 152us/step - loss: 2.6466 - val_loss: 4.0277\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 3.23768\n",
      "Epoch 125/200\n",
      "576/576 [==============================] - 0s 161us/step - loss: 2.3069 - val_loss: 3.1982\n",
      "\n",
      "Epoch 00125: val_loss improved from 3.23768 to 3.19817, saving model to best.model.hdf5\n",
      "Epoch 126/200\n",
      "576/576 [==============================] - 0s 164us/step - loss: 2.1453 - val_loss: 3.2855\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 3.19817\n",
      "Epoch 127/200\n",
      "576/576 [==============================] - 0s 173us/step - loss: 1.8063 - val_loss: 2.9219\n",
      "\n",
      "Epoch 00127: val_loss improved from 3.19817 to 2.92189, saving model to best.model.hdf5\n",
      "Epoch 128/200\n",
      "576/576 [==============================] - 0s 157us/step - loss: 1.9547 - val_loss: 3.9210\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 2.92189\n",
      "Epoch 129/200\n",
      "576/576 [==============================] - 0s 148us/step - loss: 2.2756 - val_loss: 2.9024\n",
      "\n",
      "Epoch 00129: val_loss improved from 2.92189 to 2.90237, saving model to best.model.hdf5\n",
      "Epoch 130/200\n",
      "576/576 [==============================] - 0s 161us/step - loss: 2.9922 - val_loss: 3.3300\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 2.90237\n",
      "Epoch 131/200\n",
      "576/576 [==============================] - 0s 172us/step - loss: 20.2883 - val_loss: 34.7281\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 2.90237\n",
      "Epoch 132/200\n",
      "576/576 [==============================] - 0s 155us/step - loss: 22.1887 - val_loss: 5.6795\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 2.90237\n",
      "Epoch 133/200\n",
      "576/576 [==============================] - 0s 162us/step - loss: 4.1406 - val_loss: 6.9625\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 2.90237\n",
      "Epoch 134/200\n",
      "576/576 [==============================] - 0s 150us/step - loss: 4.6844 - val_loss: 2.9232\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 2.90237\n",
      "Epoch 135/200\n",
      "576/576 [==============================] - 0s 169us/step - loss: 2.5273 - val_loss: 5.4277\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 2.90237\n",
      "Epoch 136/200\n",
      "576/576 [==============================] - 0s 162us/step - loss: 21.4667 - val_loss: 27.9676\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 2.90237\n",
      "Epoch 137/200\n",
      "576/576 [==============================] - 0s 156us/step - loss: 11.9635 - val_loss: 4.6926\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 2.90237\n",
      "Epoch 138/200\n",
      "576/576 [==============================] - 0s 158us/step - loss: 3.4299 - val_loss: 3.1354\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 2.90237\n",
      "Epoch 139/200\n",
      "576/576 [==============================] - 0s 161us/step - loss: 2.4008 - val_loss: 7.0289\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 2.90237\n",
      "Epoch 140/200\n",
      "576/576 [==============================] - 0s 149us/step - loss: 2.7527 - val_loss: 3.2245\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 2.90237\n",
      "Epoch 141/200\n",
      "576/576 [==============================] - 0s 171us/step - loss: 6.6580 - val_loss: 5.9896\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 2.90237\n",
      "Epoch 142/200\n",
      "576/576 [==============================] - 0s 145us/step - loss: 2.9113 - val_loss: 5.8081\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 2.90237\n",
      "Epoch 143/200\n",
      "576/576 [==============================] - 0s 150us/step - loss: 3.3799 - val_loss: 3.7902\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 2.90237\n",
      "Epoch 144/200\n",
      "576/576 [==============================] - 0s 152us/step - loss: 2.0118 - val_loss: 3.2375\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 2.90237\n",
      "Epoch 145/200\n",
      "576/576 [==============================] - 0s 151us/step - loss: 1.7096 - val_loss: 3.6283\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 2.90237\n",
      "Epoch 146/200\n",
      "576/576 [==============================] - 0s 169us/step - loss: 1.5445 - val_loss: 2.8362\n",
      "\n",
      "Epoch 00146: val_loss improved from 2.90237 to 2.83621, saving model to best.model.hdf5\n",
      "Epoch 147/200\n",
      "576/576 [==============================] - 0s 147us/step - loss: 1.8058 - val_loss: 2.7134\n",
      "\n",
      "Epoch 00147: val_loss improved from 2.83621 to 2.71338, saving model to best.model.hdf5\n",
      "Epoch 148/200\n",
      "576/576 [==============================] - 0s 157us/step - loss: 1.7136 - val_loss: 2.7364\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 2.71338\n",
      "Epoch 149/200\n",
      "576/576 [==============================] - 0s 164us/step - loss: 2.8146 - val_loss: 3.4743\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 2.71338\n",
      "Epoch 150/200\n",
      "576/576 [==============================] - 0s 153us/step - loss: 3.4109 - val_loss: 6.0445\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 2.71338\n",
      "Epoch 151/200\n",
      "576/576 [==============================] - 0s 189us/step - loss: 4.5149 - val_loss: 2.8281\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 2.71338\n",
      "Epoch 152/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 161us/step - loss: 2.4202 - val_loss: 3.3184\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 2.71338\n",
      "Epoch 153/200\n",
      "576/576 [==============================] - 0s 221us/step - loss: 2.5456 - val_loss: 5.6373\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 2.71338\n",
      "Epoch 154/200\n",
      "576/576 [==============================] - 0s 209us/step - loss: 3.0406 - val_loss: 2.7686\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 2.71338\n",
      "Epoch 155/200\n",
      "576/576 [==============================] - 0s 176us/step - loss: 4.2462 - val_loss: 4.4157\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 2.71338\n",
      "Epoch 156/200\n",
      "576/576 [==============================] - 0s 151us/step - loss: 2.0665 - val_loss: 3.9795\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 2.71338\n",
      "Epoch 157/200\n",
      "576/576 [==============================] - 0s 170us/step - loss: 1.7517 - val_loss: 2.7756\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 2.71338\n",
      "Epoch 158/200\n",
      "576/576 [==============================] - 0s 168us/step - loss: 2.0968 - val_loss: 5.4265\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 2.71338\n",
      "Epoch 159/200\n",
      "576/576 [==============================] - 0s 156us/step - loss: 3.7211 - val_loss: 2.5564\n",
      "\n",
      "Epoch 00159: val_loss improved from 2.71338 to 2.55640, saving model to best.model.hdf5\n",
      "Epoch 160/200\n",
      "576/576 [==============================] - 0s 152us/step - loss: 1.7226 - val_loss: 4.3324\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 2.55640\n",
      "Epoch 161/200\n",
      "576/576 [==============================] - 0s 159us/step - loss: 2.4819 - val_loss: 3.0566\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 2.55640\n",
      "Epoch 162/200\n",
      "576/576 [==============================] - 0s 152us/step - loss: 2.1208 - val_loss: 3.3108\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 2.55640\n",
      "Epoch 163/200\n",
      "576/576 [==============================] - 0s 166us/step - loss: 2.0171 - val_loss: 5.1110\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 2.55640\n",
      "Epoch 164/200\n",
      "576/576 [==============================] - 0s 153us/step - loss: 5.6411 - val_loss: 11.8078\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 2.55640\n",
      "Epoch 165/200\n",
      "576/576 [==============================] - 0s 157us/step - loss: 4.2817 - val_loss: 4.6722\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 2.55640\n",
      "Epoch 166/200\n",
      "576/576 [==============================] - 0s 157us/step - loss: 2.6556 - val_loss: 3.5574\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 2.55640\n",
      "Epoch 167/200\n",
      "576/576 [==============================] - 0s 159us/step - loss: 4.4756 - val_loss: 4.8239\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 2.55640\n",
      "Epoch 168/200\n",
      "576/576 [==============================] - 0s 166us/step - loss: 3.6267 - val_loss: 17.2065\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 2.55640\n",
      "Epoch 169/200\n",
      "576/576 [==============================] - 0s 175us/step - loss: 153.4309 - val_loss: 378.7579\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 2.55640\n",
      "Epoch 170/200\n",
      "576/576 [==============================] - 0s 238us/step - loss: 370.3351 - val_loss: 159.4003\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 2.55640\n",
      "Epoch 171/200\n",
      "576/576 [==============================] - 0s 257us/step - loss: 197.2122 - val_loss: 58.3660\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 2.55640\n",
      "Epoch 172/200\n",
      "576/576 [==============================] - 0s 182us/step - loss: 139.9699 - val_loss: 50.0692\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 2.55640\n",
      "Epoch 173/200\n",
      "576/576 [==============================] - 0s 166us/step - loss: 25.6197 - val_loss: 11.1445\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 2.55640\n",
      "Epoch 174/200\n",
      "576/576 [==============================] - 0s 254us/step - loss: 7.0104 - val_loss: 5.0949\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 2.55640\n",
      "Epoch 175/200\n",
      "576/576 [==============================] - 0s 191us/step - loss: 4.9754 - val_loss: 4.2045\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 2.55640\n",
      "Epoch 176/200\n",
      "576/576 [==============================] - 0s 165us/step - loss: 10.2947 - val_loss: 17.6822\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 2.55640\n",
      "Epoch 177/200\n",
      "576/576 [==============================] - 0s 138us/step - loss: 19.7141 - val_loss: 25.7207\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 2.55640\n",
      "Epoch 178/200\n",
      "576/576 [==============================] - 0s 166us/step - loss: 11.0928 - val_loss: 5.5823\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 2.55640\n",
      "Epoch 179/200\n",
      "576/576 [==============================] - 0s 149us/step - loss: 3.7800 - val_loss: 5.5505\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 2.55640\n",
      "Epoch 180/200\n",
      "576/576 [==============================] - 0s 152us/step - loss: 2.8435 - val_loss: 4.0408\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 2.55640\n",
      "Epoch 181/200\n",
      "576/576 [==============================] - 0s 157us/step - loss: 5.5928 - val_loss: 13.4793\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 2.55640\n",
      "Epoch 182/200\n",
      "576/576 [==============================] - 0s 159us/step - loss: 8.4117 - val_loss: 3.7176\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 2.55640\n",
      "Epoch 183/200\n",
      "576/576 [==============================] - 0s 151us/step - loss: 2.9387 - val_loss: 3.9829\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 2.55640\n",
      "Epoch 184/200\n",
      "576/576 [==============================] - 0s 162us/step - loss: 2.3433 - val_loss: 3.4598\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 2.55640\n",
      "Epoch 185/200\n",
      "576/576 [==============================] - 0s 158us/step - loss: 2.2723 - val_loss: 3.3161\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 2.55640\n",
      "Epoch 186/200\n",
      "576/576 [==============================] - 0s 151us/step - loss: 2.1657 - val_loss: 3.3951\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 2.55640\n",
      "Epoch 187/200\n",
      "576/576 [==============================] - 0s 135us/step - loss: 2.7914 - val_loss: 4.5357\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 2.55640\n",
      "Epoch 188/200\n",
      "576/576 [==============================] - 0s 152us/step - loss: 3.4539 - val_loss: 4.0645\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 2.55640\n",
      "Epoch 189/200\n",
      "576/576 [==============================] - 0s 145us/step - loss: 4.0519 - val_loss: 3.4265\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 2.55640\n",
      "Epoch 190/200\n",
      "576/576 [==============================] - 0s 155us/step - loss: 3.0526 - val_loss: 3.2522\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 2.55640\n",
      "Epoch 191/200\n",
      "576/576 [==============================] - 0s 164us/step - loss: 2.4009 - val_loss: 3.2506\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 2.55640\n",
      "Epoch 192/200\n",
      "576/576 [==============================] - 0s 153us/step - loss: 2.0642 - val_loss: 3.7415\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 2.55640\n",
      "Epoch 193/200\n",
      "576/576 [==============================] - 0s 152us/step - loss: 2.3877 - val_loss: 3.2380\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 2.55640\n",
      "Epoch 194/200\n",
      "576/576 [==============================] - 0s 159us/step - loss: 2.2346 - val_loss: 3.7314\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 2.55640\n",
      "Epoch 195/200\n",
      "576/576 [==============================] - 0s 166us/step - loss: 3.4324 - val_loss: 4.4948\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 2.55640\n",
      "Epoch 196/200\n",
      "576/576 [==============================] - 0s 152us/step - loss: 2.0998 - val_loss: 3.0891\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 2.55640\n",
      "Epoch 197/200\n",
      "576/576 [==============================] - 0s 181us/step - loss: 2.3717 - val_loss: 3.0690\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 2.55640\n",
      "Epoch 198/200\n",
      "576/576 [==============================] - 0s 147us/step - loss: 3.0233 - val_loss: 5.8788\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 2.55640\n",
      "Epoch 199/200\n",
      "576/576 [==============================] - 0s 160us/step - loss: 2.7614 - val_loss: 3.4242\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 2.55640\n",
      "Epoch 200/200\n",
      "576/576 [==============================] - 0s 180us/step - loss: 2.4329 - val_loss: 2.9694\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 2.55640\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5e30070128>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "# Cross Validation scores\n",
    "# Fit the model\n",
    "checkpointer = ModelCheckpoint(filepath='best.model.hdf5',verbose=1,save_best_only=True)\n",
    "model.fit(X_train, Y_train, validation_split=0.2,epochs=200, callbacks=[checkpointer])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAD8CAYAAABQFVIjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRUREYkKBIiIiMaFAERGRmPj/OQ0Ti/0E9KgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff253340a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "hw0 = 20.0\n",
    "hw1 = 30.0\n",
    "hw2 = 40.0\n",
    "Nmax = np.arange(1,350,1)/np.max(Nmax)\n",
    "y_func0 = np.asarray([func(hw0,ni) for ni in Nmax  ])\n",
    "y_func1 = np.asarray([func(hw0,ni) for ni in Nmax  ])\n",
    "y_func2 = np.asarray([func(hw0,ni) for ni in Nmax  ])\n",
    "y_pred0 = np.asarray([model.predict(np.asarray([[hw0,ni]]))[0] for ni in Nmax  ])\n",
    "y_pred1 = np.asarray([model.predict(np.asarray([[hw1,ni]]))[0] for ni in Nmax  ])\n",
    "y_pred2 = np.asarray([model.predict(np.asarray([[hw2,ni]]))[0] for ni in Nmax  ])\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(Nmax,y_func0)\n",
    "#plt.plot(Nmax,y_func1)\n",
    "#plt.plot(Nmax,y_func2)\n",
    "plt.plot(Nmax,y_pred0)\n",
    "plt.plot(Nmax,y_pred1)\n",
    "plt.plot(Nmax,y_pred2)\n",
    "plt.xlim([0,300])\n",
    "#plt.ylim([9.8,10.2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
