{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data (73, 1)\n",
      "ymax 35.34\n",
      "ymin 16.93\n",
      "ymax 38.1\n",
      "ymin 17.69\n"
     ]
    }
   ],
   "source": [
    "# First we read in the airline passenger\n",
    "#fileName = \"data/international-airline-passengers.csv\"\n",
    "#fileName = \"data/wolfer-sunspot-numbers-1770-to-1.csv\"\n",
    "#fileName ='data/EURUSD_15m_BID_01.01.2010-31.12.2016.csv'\n",
    "fileName ='data/total-annual-rainfall-in-inches-.csv'\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(fileName, engine='python', skipfooter=3)\n",
    "\n",
    "\n",
    "if(fileName=='data/international-airline-passengers.csv'):\n",
    "    time_column_name = 'Month'\n",
    "elif(fileName=='data/wolfer-sunspot-numbers-1770-to-1.csv'):\n",
    "    time_column_name='Year'\n",
    "elif(fileName=='data/EURUSD_15m_BID_01.01.2010-31.12.2016.csv'):\n",
    "    time_column_name='Time'\n",
    "    df = df[['Time', 'Close']].copy()\n",
    "elif(fileName=='data/total-annual-rainfall-in-inches-.csv'):\n",
    "    time_column_name='Year'\n",
    "\n",
    "df[time_column_name]=pd.to_datetime(df[time_column_name], format='%Y-%m-%d')\n",
    "df.set_index([time_column_name], inplace=True)\n",
    "\n",
    "\n",
    "if(fileName=='data/EURUSD_15m_BID_01.01.2010-31.12.2016.csv'):\n",
    "    train_data = df['2016-01-01 00:00':'2016-05-01 00:00']\n",
    "    #train_data = df['2016-04-01 00:00':'2016-05-01 00:00']\n",
    "    #train_data = df['2016-04-27 00:00':'2016-05-01 00:00']\n",
    "    test_data = df['2016-05-01 00:15':'2016-06-30 23:45']\n",
    "    \n",
    "    time_vals = df[train_data.index[0]:test_data.index[len(test_data)-1]]\n",
    "    train_data = train_data.values.astype('float32')\n",
    "    test_data = test_data.values.astype('float32')\n",
    "    number_of_epochs=10\n",
    "    \n",
    "    # Extract the raw data, without the dates\n",
    "elif(fileName=='data/wolfer-sunspot-numbers-1770-to-1.csv'):\n",
    "    time_vals = df[df.index[0]:df.index[len(df)-1]]\n",
    "    data = df.values\n",
    "    data = data.astype('float32')\n",
    "    \n",
    "    # Now we split the data and apply the data scalar\n",
    "    split = 71\n",
    "    train_data =  data[0:split,:]\n",
    "    test_data = data[split:,:]\n",
    "    number_of_epochs=300\n",
    "    \n",
    "    \n",
    "elif(fileName=='data/international-airline-passengers.csv'):\n",
    "    time_vals = df[df.index[0]:df.index[len(df)-1]]\n",
    "    data = df.values\n",
    "    data = data.astype('float32')\n",
    "    \n",
    "    # Now we split the data and apply the data scalar\n",
    "    split = 73\n",
    "    train_data =  data[0:split,:]\n",
    "    test_data = data[split:,:]\n",
    "    number_of_epochs=300\n",
    "\n",
    "elif(fileName=='data/total-annual-rainfall-in-inches-.csv'):\n",
    "    time_vals = df[df.index[0]:df.index[len(df)-1]]\n",
    "    data = df.values\n",
    "    data = data.astype('float32')\n",
    "    \n",
    "    # Now we split the data and apply the data scalar\n",
    "    split = 73\n",
    "    train_data =  data[0:split,:]\n",
    "    test_data = data[split:,:]\n",
    "    number_of_epochs=300\n",
    "\n",
    "\n",
    "print(\"train data\", train_data.shape)\n",
    "\n",
    "def future_data(data,lags=1,future=1):\n",
    "    X, y = [], []\n",
    "    for row in range(len(data) - lags - future):\n",
    "        a = data[row:(row + lags), 0]\n",
    "        X.append(a)\n",
    "        y.append(data[row + lags+future-1, 0])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def scale_array(y_vec):\n",
    "    \n",
    "    ymax = np.max(y_vec)\n",
    "    ymin = np.min(y_vec)\n",
    "    \n",
    "    print(\"ymax\",ymax)\n",
    "    print(\"ymin\",ymin)\n",
    "    \n",
    "    y_vec_scaled = np.zeros((len(y_vec),1))\n",
    "    \n",
    "    for k in range(0,len(y_vec)):\n",
    "        y_vec_scaled[k][0] = (y_vec[k][0]-ymin)/(ymax-ymin)\n",
    "    \n",
    "    return y_vec_scaled,ymin,ymax\n",
    "\n",
    "def invert_scaling(y_vec_scaled,ymin,ymax):\n",
    "    \n",
    "    y_vec = np.zeros(y_vec_scaled.shape)\n",
    "        \n",
    "        \n",
    "    for k in range(0,len(y_vec_scaled)):\n",
    "        y_vec[k] = ymin+(ymax-ymin)*y_vec_scaled[k]\n",
    "    \n",
    "    return y_vec\n",
    "\n",
    "# Process the train and test data\n",
    "train_data,train_min,train_max = scale_array(train_data)\n",
    "test_data,test_min,test_max = scale_array(test_data)\n",
    "\n",
    "lags=1\n",
    "future=1\n",
    "\n",
    "# Process the training data\n",
    "X_train, y_train = future_data(train_data, lags,future)\n",
    "X_test, y_test = future_data(test_data, lags,future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.1808\n",
      "Epoch 2/300\n",
      "71/71 [==============================] - 0s 992us/step - loss: 0.1428\n",
      "Epoch 3/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1128\n",
      "Epoch 4/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0912\n",
      "Epoch 5/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0780\n",
      "Epoch 6/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0694\n",
      "Epoch 7/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0655\n",
      "Epoch 8/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0636\n",
      "Epoch 9/300\n",
      "71/71 [==============================] - 0s 993us/step - loss: 0.0625\n",
      "Epoch 10/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0623\n",
      "Epoch 11/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0620\n",
      "Epoch 12/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0618\n",
      "Epoch 13/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0617\n",
      "Epoch 14/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0613\n",
      "Epoch 15/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0611\n",
      "Epoch 16/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0609\n",
      "Epoch 17/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0607\n",
      "Epoch 18/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0605\n",
      "Epoch 19/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0604\n",
      "Epoch 20/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0603\n",
      "Epoch 21/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0604\n",
      "Epoch 22/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0600\n",
      "Epoch 23/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0598\n",
      "Epoch 24/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0597\n",
      "Epoch 25/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0597\n",
      "Epoch 26/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0596\n",
      "Epoch 27/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0595\n",
      "Epoch 28/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0592\n",
      "Epoch 29/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0593\n",
      "Epoch 30/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0590\n",
      "Epoch 31/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0588\n",
      "Epoch 32/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0587\n",
      "Epoch 33/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0587\n",
      "Epoch 34/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0586\n",
      "Epoch 35/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0586\n",
      "Epoch 36/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0583\n",
      "Epoch 37/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0586\n",
      "Epoch 38/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0583\n",
      "Epoch 39/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0582\n",
      "Epoch 40/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0583\n",
      "Epoch 41/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0580\n",
      "Epoch 42/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0579\n",
      "Epoch 43/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0579\n",
      "Epoch 44/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0577\n",
      "Epoch 45/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0577\n",
      "Epoch 46/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0576\n",
      "Epoch 47/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0576\n",
      "Epoch 48/300\n",
      "71/71 [==============================] - 0s 995us/step - loss: 0.0575\n",
      "Epoch 49/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0577\n",
      "Epoch 50/300\n",
      "71/71 [==============================] - 0s 979us/step - loss: 0.0575\n",
      "Epoch 51/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0577\n",
      "Epoch 52/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0573\n",
      "Epoch 53/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0573\n",
      "Epoch 54/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0572\n",
      "Epoch 55/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0573\n",
      "Epoch 56/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0571\n",
      "Epoch 57/300\n",
      "71/71 [==============================] - 0s 949us/step - loss: 0.0570\n",
      "Epoch 58/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0571\n",
      "Epoch 59/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0574\n",
      "Epoch 60/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0569\n",
      "Epoch 61/300\n",
      "71/71 [==============================] - 0s 992us/step - loss: 0.0569\n",
      "Epoch 62/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0569\n",
      "Epoch 63/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0569\n",
      "Epoch 64/300\n",
      "71/71 [==============================] - 0s 984us/step - loss: 0.0570\n",
      "Epoch 65/300\n",
      "71/71 [==============================] - 0s 979us/step - loss: 0.0568\n",
      "Epoch 66/300\n",
      "71/71 [==============================] - 0s 968us/step - loss: 0.0567\n",
      "Epoch 67/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0566\n",
      "Epoch 68/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0566\n",
      "Epoch 69/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0567\n",
      "Epoch 70/300\n",
      "71/71 [==============================] - 0s 954us/step - loss: 0.0567\n",
      "Epoch 71/300\n",
      "71/71 [==============================] - 0s 993us/step - loss: 0.0569\n",
      "Epoch 72/300\n",
      "71/71 [==============================] - 0s 916us/step - loss: 0.0565\n",
      "Epoch 73/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0565\n",
      "Epoch 74/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0565\n",
      "Epoch 75/300\n",
      "71/71 [==============================] - 0s 979us/step - loss: 0.0569\n",
      "Epoch 76/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0565\n",
      "Epoch 77/300\n",
      "71/71 [==============================] - 0s 992us/step - loss: 0.0565\n",
      "Epoch 78/300\n",
      "71/71 [==============================] - 0s 973us/step - loss: 0.0566\n",
      "Epoch 79/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0563\n",
      "Epoch 80/300\n",
      "71/71 [==============================] - 0s 991us/step - loss: 0.0564\n",
      "Epoch 81/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0564\n",
      "Epoch 82/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0564\n",
      "Epoch 83/300\n",
      "71/71 [==============================] - 0s 954us/step - loss: 0.0563\n",
      "Epoch 84/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0569\n",
      "Epoch 85/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0562\n",
      "Epoch 86/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0562\n",
      "Epoch 87/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0560\n",
      "Epoch 88/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0562\n",
      "Epoch 89/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0561\n",
      "Epoch 90/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0561\n",
      "Epoch 91/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0561\n",
      "Epoch 92/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0561\n",
      "Epoch 93/300\n",
      "71/71 [==============================] - 0s 989us/step - loss: 0.0561\n",
      "Epoch 94/300\n",
      "71/71 [==============================] - 0s 936us/step - loss: 0.0561\n",
      "Epoch 95/300\n",
      "71/71 [==============================] - 0s 965us/step - loss: 0.0560\n",
      "Epoch 96/300\n",
      "71/71 [==============================] - 0s 995us/step - loss: 0.0559\n",
      "Epoch 97/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0560\n",
      "Epoch 98/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0558\n",
      "Epoch 99/300\n",
      "71/71 [==============================] - 0s 955us/step - loss: 0.0559\n",
      "Epoch 100/300\n",
      "71/71 [==============================] - 0s 980us/step - loss: 0.0560\n",
      "Epoch 101/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0560\n",
      "Epoch 102/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0559\n",
      "Epoch 103/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0559\n",
      "Epoch 104/300\n",
      "71/71 [==============================] - 0s 992us/step - loss: 0.0559\n",
      "Epoch 105/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0558\n",
      "Epoch 106/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0563\n",
      "Epoch 107/300\n",
      "71/71 [==============================] - 0s 996us/step - loss: 0.0559\n",
      "Epoch 108/300\n",
      "71/71 [==============================] - 0s 942us/step - loss: 0.0558\n",
      "Epoch 109/300\n",
      "71/71 [==============================] - 0s 980us/step - loss: 0.0559\n",
      "Epoch 110/300\n",
      "71/71 [==============================] - 0s 951us/step - loss: 0.0558\n",
      "Epoch 111/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0559\n",
      "Epoch 112/300\n",
      "71/71 [==============================] - 0s 950us/step - loss: 0.0557\n",
      "Epoch 113/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0558\n",
      "Epoch 114/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0557\n",
      "Epoch 115/300\n",
      "71/71 [==============================] - 0s 919us/step - loss: 0.0558\n",
      "Epoch 116/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0558\n",
      "Epoch 117/300\n",
      "71/71 [==============================] - 0s 989us/step - loss: 0.0558\n",
      "Epoch 118/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0558\n",
      "Epoch 119/300\n",
      "71/71 [==============================] - 0s 968us/step - loss: 0.0558\n",
      "Epoch 120/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0557\n",
      "Epoch 121/300\n",
      "71/71 [==============================] - 0s 999us/step - loss: 0.0557\n",
      "Epoch 122/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0557\n",
      "Epoch 123/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0558\n",
      "Epoch 124/300\n",
      "71/71 [==============================] - 0s 966us/step - loss: 0.0556\n",
      "Epoch 125/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0556\n",
      "Epoch 126/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0557\n",
      "Epoch 127/300\n",
      "71/71 [==============================] - 0s 952us/step - loss: 0.0561\n",
      "Epoch 128/300\n",
      "71/71 [==============================] - 0s 962us/step - loss: 0.0556\n",
      "Epoch 129/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0557\n",
      "Epoch 130/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0555\n",
      "Epoch 131/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0556\n",
      "Epoch 132/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0556\n",
      "Epoch 133/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0555\n",
      "Epoch 134/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0557\n",
      "Epoch 135/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0558\n",
      "Epoch 136/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0555\n",
      "Epoch 137/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0555\n",
      "Epoch 138/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0556\n",
      "Epoch 139/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0556\n",
      "Epoch 140/300\n",
      "71/71 [==============================] - 0s 955us/step - loss: 0.0556\n",
      "Epoch 141/300\n",
      "71/71 [==============================] - 0s 957us/step - loss: 0.0554\n",
      "Epoch 142/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0556\n",
      "Epoch 143/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0559\n",
      "Epoch 144/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0555\n",
      "Epoch 145/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0554\n",
      "Epoch 146/300\n",
      "71/71 [==============================] - 0s 887us/step - loss: 0.0555\n",
      "Epoch 147/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0556\n",
      "Epoch 148/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0559\n",
      "Epoch 149/300\n",
      "71/71 [==============================] - 0s 979us/step - loss: 0.0556\n",
      "Epoch 150/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0554\n",
      "Epoch 151/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0555\n",
      "Epoch 152/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0554\n",
      "Epoch 153/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0554\n",
      "Epoch 154/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0556\n",
      "Epoch 155/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0556\n",
      "Epoch 156/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0554\n",
      "Epoch 157/300\n",
      "71/71 [==============================] - 0s 878us/step - loss: 0.0555\n",
      "Epoch 158/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0554\n",
      "Epoch 159/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0554\n",
      "Epoch 160/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0554\n",
      "Epoch 161/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0553\n",
      "Epoch 162/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0554\n",
      "Epoch 163/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0554\n",
      "Epoch 164/300\n",
      "71/71 [==============================] - 0s 968us/step - loss: 0.0554\n",
      "Epoch 165/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0554\n",
      "Epoch 166/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0555\n",
      "Epoch 167/300\n",
      "71/71 [==============================] - 0s 975us/step - loss: 0.0557\n",
      "Epoch 168/300\n",
      "71/71 [==============================] - 0s 981us/step - loss: 0.0554\n",
      "Epoch 169/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0557\n",
      "Epoch 170/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0553\n",
      "Epoch 171/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0556\n",
      "Epoch 172/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0555\n",
      "Epoch 173/300\n",
      "71/71 [==============================] - 0s 968us/step - loss: 0.0554\n",
      "Epoch 174/300\n",
      "71/71 [==============================] - 0s 937us/step - loss: 0.0554\n",
      "Epoch 175/300\n",
      "71/71 [==============================] - 0s 932us/step - loss: 0.0554\n",
      "Epoch 176/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0554\n",
      "Epoch 177/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0553\n",
      "Epoch 178/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0554\n",
      "Epoch 179/300\n",
      "71/71 [==============================] - 0s 920us/step - loss: 0.0555\n",
      "Epoch 180/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0554\n",
      "Epoch 181/300\n",
      "71/71 [==============================] - 0s 988us/step - loss: 0.0554\n",
      "Epoch 182/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0556\n",
      "Epoch 183/300\n",
      "71/71 [==============================] - 0s 957us/step - loss: 0.0557\n",
      "Epoch 184/300\n",
      "71/71 [==============================] - 0s 936us/step - loss: 0.0552\n",
      "Epoch 185/300\n",
      "71/71 [==============================] - 0s 969us/step - loss: 0.0553\n",
      "Epoch 186/300\n",
      "71/71 [==============================] - 0s 981us/step - loss: 0.0554\n",
      "Epoch 187/300\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0553\n",
      "Epoch 188/300\n",
      " 2/71 [..............................] - ETA: 0s - loss: 0.0530"
     ]
    }
   ],
   "source": [
    "# create and fit Feed forward neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(4, input_dim=lags, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "history=model.fit(X_train, y_train, epochs=number_of_epochs, batch_size=2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())\n",
    "\n",
    "\n",
    "if(fileName=='data/international-airline-passengers.csv'):\n",
    "    loss_label = 'Loss_airline.pdf'\n",
    "elif(fileName=='data/wolfer-sunspot-numbers-1770-to-1.csv'):\n",
    "    loss_label = 'Loss_sunspot.pdf'\n",
    "elif(fileName=='data/EURUSD_15m_BID_01.01.2010-31.12.2016.csv'):\n",
    "    loss_label = 'Loss_EURUSD.pdf'\n",
    "elif(fileName=='data/total-annual-rainfall-in-inches-.csv'):\n",
    "    loss_label = 'Loss_EURUSD.pdf'\n",
    "\n",
    "plt.plot()\n",
    "plt.plot(history.history['loss'],color='b')\n",
    "plt.ylabel('Loss',fontsize=15)\n",
    "plt.xlabel('Epoch',fontsize=20)\n",
    "plt.savefig(loss_label,bboxes='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions for training\n",
    "uncertainty_samples = 100\n",
    "\n",
    "train_predict = model.predict(X_train)\n",
    "test_predict = model.predict(X_test)\n",
    "\n",
    "print(test_predict.shape)\n",
    "\n",
    "residuals = np.abs(y_test.reshape(-1, 1) - test_predict.reshape(-1, 1))\n",
    "\n",
    "# Here we estimate the average residual \n",
    "sigma = 0.5*np.mean(residuals)\n",
    "std_sigma =  0.5*np.std(residuals)\n",
    "print(\"Estimated Sigma: \", sigma, np.std(residuals))\n",
    "\n",
    "plt.plot(y_train)\n",
    "plt.plot(train_predict)\n",
    "\n",
    "\n",
    "test_predictions = []\n",
    "for k in range(0,uncertainty_samples):\n",
    "    test_predict = model.predict(X_test)+np.random.normal(sigma, std_sigma, test_predict.shape)\n",
    "    test_predictions.append(test_predict)\n",
    "\n",
    "test_predictions = np.asarray(test_predictions)\n",
    "\n",
    "\n",
    "\n",
    "y = test_predict.flatten()[future:]\n",
    "\n",
    "def return_min_max_prediction(test_predictions,i):\n",
    "    \n",
    "    y_vec = [ test_predictions[k][i][0] for k in range(0,test_predictions.shape[0])]\n",
    "    \n",
    "    ymin = np.min(y_vec)\n",
    "    ymax = np.max(y_vec)\n",
    "    \n",
    "    return ymin,ymax\n",
    "\n",
    "print(test_predictions.shape)\n",
    "y_min = np.asarray([return_min_max_prediction(test_predictions,i)[0] for i in range(0,len(y))])\n",
    "y_max = np.asarray([return_min_max_prediction(test_predictions,i)[1] for i in range(0,len(y))])\n",
    "x = np.arange(-future,len(y_min)-future)\n",
    "\n",
    "\n",
    "# Unscale all of the data\n",
    "#print(y_test.shape)\n",
    "y_test = invert_scaling(y_test,test_min,test_max)\n",
    "y_min = invert_scaling(y_min,test_min,test_max)\n",
    "y_max = invert_scaling(y_max,test_min,test_max)\n",
    "y = invert_scaling(y,test_min,test_max)\n",
    "\n",
    "\n",
    "# Compute the root mean squared error\n",
    "mse = ((y_test[:-1].reshape(-1, 1) - y.reshape(-1, 1)) ** 2).mean()\n",
    "\n",
    "\n",
    "print('-------------------------------------')\n",
    "print(\"MSE: \", mse)\n",
    "print(\"RMSE: \", np.sqrt(mse))\n",
    "print('-------------------------------------')\n",
    "\n",
    "\n",
    "#============================================================================================\n",
    "# Unscale data\n",
    "#test_predictions = scaler.inverse_transform(test_predictions)\n",
    "y_train = invert_scaling(y_train,train_min,train_max)\n",
    "train_predict = invert_scaling(train_predict,train_min,train_max)\n",
    "\n",
    "\n",
    "x_test = range(len(y_train),len(y_train)+len(y_test))\n",
    "x = range(len(y_train),len(y_train)+len(y))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if(fileName=='data/international-airline-passengers.csv'):\n",
    "    time_steps = 15\n",
    "    time_format= \"%Y\"\n",
    "    y_label = 'Number of Airline Passengers'\n",
    "    output_name = 'Airline_Passengers_NN.pdf'\n",
    "elif(fileName=='data/wolfer-sunspot-numbers-1770-to-1.csv'):\n",
    "    time_steps = 10\n",
    "    time_format= \"%Y\"\n",
    "    y_label = 'Number of Sunspots'\n",
    "    output_name = 'Sunspot_Forecast_NN.pdf'\n",
    "elif(fileName=='data/EURUSD_15m_BID_01.01.2010-31.12.2016.csv'):\n",
    "    time_format= \"%m-%H:%M\"\n",
    "    time_steps = 2800\n",
    "    y_label = 'Exchange Rate EUR/USD'\n",
    "    output_name = 'EURO_USD_exchange_rate_NN.pdf'\n",
    "elif(fileName=='data/total-annual-rainfall-in-inches-.csv'):\n",
    "    time_format= \"%Y\"\n",
    "    time_steps = 28\n",
    "    y_label = 'Annual Rainfall [Inches]'\n",
    "    output_name = 'EURO_USD_exchange_rate_NN.pdf'\n",
    "\n",
    "vals= range(0,len(y_train)+len(y_test),time_steps)\n",
    "labels = [time_vals.index[k].strftime(time_format) for k in vals]\n",
    "\n",
    "\n",
    "plt.xticks(vals, labels)\n",
    "plt.ylabel(y_label,fontsize=15)\n",
    "plt.xlabel('Date',fontsize=20)\n",
    "plt.plot(y_train, label='Training data',color='b');\n",
    "plt.plot(x_test,y_test.reshape(-1, 1), label='Test Data', color='g')\n",
    "plt.plot(x,y, label='Test Prediction', color='red')\n",
    "plt.fill_between(x, y_min, y_max, facecolor='red', interpolate=True,alpha=0.3)\n",
    "plt.legend(loc=2)\n",
    "plt.savefig(output_name,bboxes='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
